## Overview

| Developed by | Guardrails AI |
| --- | --- |
| Date of development | Feb 15, 2024 |
| Validator type | RAG |
| Blog | https://www.guardrailsai.com/blog/reduce-ai-hallucinations-provenance-guardrails |
| License | Apache 2 |
| Input/Output | Output |

## Description

This validator uses an LLM callable to evaluate the generated text against the provided contexts. In order to use this validator, you must provide one of the following in the `metadata` field while calling the validator:
- A `query_function`: `query_function` takes the LLM-generated text string as input and returns a list of relevant chunks. The list should be sorted in ascending order by the distance between the chunk and the LLM-generated text.
- `sources` and `embed_function`: `sources` is a list of strings containing the text that the LLM attribute is attributed against. The `embed_function` should take a string or a list of strings as input and return a np array of floats. The vector should be normalized to unit length.

Below is a step-wise breakdown of how the validator works:

1. The list of sources is chunked based on user's parameters.
2. Each source chunk is embedded and stored in a vector database or an in-memory embedding store.
3. The LLM generated output is chunked based on user-specified parameters.
4. Each LLM output chunk is embedded by the same model used for embedding source chunks.
5. The `k` nearest source chunks are determined for the LLM output chunk.
6. To evaluate whether the `k` nearest source chunks support the LLM output chunk, a call to an LLM is made to get a prediction.

### Intended use

The primary intended use is for RAG applications to check if a text is hallucinated by establishing a source (i.e. provenance) for any LLM generated text. Out-of-scope use cases are general question answering without RAG or text grounding.

### Resources required

- Dependencies: Foundation model library, Embedding library
- Foundation model access keys: Yes (depending on which model is used)

## Installation

```bash
$ gudardrails hub install hub://guardrails/provenance_llm
```

## Usage Examples

### Validating string output via Python

In this example, we apply the validator to a string output generated by an LLM.

```python
# Import Guard and Validator
from guardrails.hub import ProvenanceLLM
from guardrails import Guard

# Import embedding model
from sentence_transformers import SentenceTransformer

# Initialize Validator
val = ProvenanceLLM(
    validation_method="sentence",
    llm_callable="gpt-3.5-turbo",
    top_k=3,
    max_tokens=2,
)

# Setup Guard
guard = Guard.from_string(validators=[val])

# Setup text sources
sources = [
    "The sun is a star.",
    "The sun rises in the east and sets in the west."
]

# Load model for embedding function
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Create embed function
def embed_function(sources: list[str]) -> np.array:
    return model.encode(sources)

guard.parse(
    llm_output="The sun rises in the east.",
    metadata={
        "sources": sources,
        "embed_function": embed_function
    }
)
```

### Validating JSON output via Python

In this example, we apply the validator to a string field of a JSON output generated by an LLM.

```python
# Import Guard and Validator
from pydantic import BaseModel
from guardrails.hub import ProvenanceLLM
from guardrails import Guard

# Import embedding model
from sentence_transformers import SentenceTransformer

# Initialize Validator
val = ProvenanceLLM(
    validation_method="sentence",
    llm_callable="gpt-3.5-turbo",
    top_k=3,
    max_tokens=2,
)

# Create Pydantic BaseModel
class LLMOutput(BaseModel):
    output: str = Field(
        description="Output generated by LLM", validators=[val]
    )

# Create a Guard to check for valid Pydantic output
guard = Guard.from_pydantic(output_class=LLMOutput)

# Setup text sources
sources = [
    "The sun is a star.",
    "The sun rises in the east and sets in the west."
]

# Load model for embedding function
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Create embed function
def embed_function(sources: list[str]) -> np.array:
    return model.encode(sources)

# Run LLM output generating JSON through guard
guard.parse("""
{
    "output": "The sun rises in the east."
},
""",
    metadata={
        "sources": sources,
        "embed_function": embed_function
    }
)
```

## API Reference
`__init__`
- `validation_method`: Whether to validate at the sentence level or over the full text. Must be one of sentence or full. Defaults to sentence.
- `llm_callable`: Either the name of the OpenAI model, or a callable that takes a prompt and returns a response.
- `top_k`: The number of chunks to return from the query function. Defaults to 3.
- `max_tokens`: The maximum number of tokens to send to the LLM. Defaults to 2.

`__call__`
- `query_function` Callable, optional: A callable that takes a string and returns a list of (chunk, score) tuples. In order to use this validator, you must provide either a query_function or sources with an embed_function in the metadata. The query_function should take a string as input and return a list of (chunk, score) tuples. The chunk is a string and the score is a float representing the cosine distance between the chunk and the input string. The list should be sorted in ascending order by score.
- `sources` List[str], optional: The source text. In order to use this validator, you must provide either a query_function or sources with an embed_function in the metadata.
- `embed_function` Callable, optional: A callable that creates embeddings for the sources. Must accept a list of strings and return an np.array of floats.
